{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sakinat-Folorunso/OOU_CSC309_Artificial_Intelligence/blob/main/notebooks/CSC309_Week02_Intelligent_Agents_Student_Centred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40e430ba",
      "metadata": {
        "id": "40e430ba"
      },
      "source": [
        "# CSC309 ‚Äì Artificial Intelligence  \n",
        "**Week 2 Lab:** Intelligent Agents ‚Äî Random vs Reflex vs Model‚Äëbased\n",
        "\n",
        "**Instructor:** Dr Sakinat Folorunso\n",
        "\n",
        "**Title:** Associate Professor of AI Systems and FAIR Data **Department:** Computer Sciences, Olabisi Onabanjo University, Ago-Iwoye, Ogun State, Nigeria\n",
        "\n",
        "**Course Code:** CSC 309\n",
        "\n",
        "**Mode:** Student‚Äëcentred, hands‚Äëon in Google Colab\n",
        "\n",
        "> Every code cell is commented line‚Äëby‚Äëline so you can follow the logic precisely."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ffbb320",
      "metadata": {
        "id": "4ffbb320"
      },
      "source": [
        "## How to use this notebook\n",
        "1. Start with the **Group Log** and **Do Now**.  \n",
        "2. Run the **Setup** cell once.  \n",
        "3. Work through **Tasks**. Edit only cells marked **`# TODO(Student)`**.  \n",
        "4. Use **Quick Checks** to test your understanding.  \n",
        "5. Finish with the **Reflection**. If you finish early, try the **Extensions**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f1b6855",
      "metadata": {
        "id": "8f1b6855"
      },
      "outputs": [],
      "source": [
        "#@title üßëüèΩ‚Äçü§ù‚Äçüßëüèæ Group Log (fill before you start)\n",
        "# The '#@param' annotations create form fields in Colab for easy input.\n",
        "\n",
        "group_members = \"Type names here\"  #@param {type:\"string\"}  # Names of teammates\n",
        "roles_notes = \"Driver/Navigator, decisions, questions\"  #@param {type:\"string\"}  # Short working notes\n",
        "\n",
        "print(\"üë• Group:\", group_members)        # Echo the group list for confirmation\n",
        "print(\"üìù Notes:\", roles_notes)          # Echo the notes so they're preserved in output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f40c3630",
      "metadata": {
        "id": "f40c3630"
      },
      "source": [
        "### Learning Objectives\n",
        "- Define **PEAS**, **rationality**, and **performance measures**.  \n",
        "- Implement a small environment and three agent policies.  \n",
        "- Compare policies using score distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88155c41",
      "metadata": {
        "id": "88155c41"
      },
      "outputs": [],
      "source": [
        "# TODO(Student): Model-based agent + comparison plot (with line-by-line comments)\n",
        "#@title üß™ Environment + Policies (fully commented)\n",
        "# We implement a tiny \"vacuum-world\" style grid with dirt.\n",
        "# The agent gets +10 for cleaning a dirty cell and ‚àí1 for moving or cleaning a clean cell.\n",
        "#@title üîß Setup (run once)\n",
        "# This lab uses only common scientific Python libraries.\n",
        "# Each import line is commented to explain its role.\n",
        "\n",
        "import sys                  # Access to Python interpreter details (not strictly required)\n",
        "import subprocess           # Allows us to call 'pip' if needed\n",
        "def pip_install(pkgs):      # Helper to install packages only if missing\n",
        "    for p in pkgs:\n",
        "        try:\n",
        "            __import__(p.split(\"==\")[0])   # Try to import the package by name\n",
        "        except Exception:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", p])  # Quiet install\n",
        "\n",
        "pip_install([\"numpy\", \"matplotlib\"])       # We need NumPy for arrays and Matplotlib for plots\n",
        "\n",
        "import numpy as np           # Numerical arrays and random sampling\n",
        "import random                # Simple random choices for agent actions\n",
        "import matplotlib.pyplot as plt  # Basic plotting for histograms\n",
        "\n",
        "print(\"‚úÖ Setup complete for Week 2.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e641aa87",
      "metadata": {
        "id": "e641aa87"
      },
      "source": [
        "### Do Now\n",
        "Sketch a quick **PEAS** for a campus cleaning robot (Performance, Environment, Actuators, Sensors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "484faeee",
      "metadata": {
        "id": "484faeee"
      },
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "    def __init__(self, n=5, dirt_prob=0.3, seed=0):\n",
        "        random.seed(seed)                      # Fix Python's random seed for reproducibility\n",
        "        np.random.seed(seed)                   # Fix NumPy's random seed for reproducibility\n",
        "        self.n = n                             # Grid size (n x n)\n",
        "        self.agent_pos = (0, 0)                # Start position in the top‚Äëleft corner\n",
        "        self.dirt = (np.random.rand(n, n) < dirt_prob).astype(int)  # 1 indicates dirt; 0 is clean\n",
        "        self.score = 0                         # Cumulative score earned by the agent\n",
        "\n",
        "    def perceive(self):\n",
        "        x, y = self.agent_pos                  # Unpack the current coordinates\n",
        "        return {\"dirty\": bool(self.dirt[x, y])}# Observation: is the current cell dirty?\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.agent_pos                  # Current position of the agent\n",
        "        if action == \"CLEAN\":                  # If the agent chooses to clean\n",
        "            if self.dirt[x, y] == 1:          # Check if the current cell actually has dirt\n",
        "                self.dirt[x, y] = 0           # Remove the dirt\n",
        "                self.score += 10              # Reward for cleaning dirt\n",
        "            else:\n",
        "                self.score -= 1               # Penalty for cleaning when there is no dirt\n",
        "        elif action in [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]:  # If the agent chooses to move\n",
        "            nx, ny = x, y                     # Start with the current position\n",
        "            if action == \"UP\":   nx = max(0, x - 1)              # Move up, staying inside the grid\n",
        "            if action == \"DOWN\": nx = min(self.n - 1, x + 1)     # Move down, staying inside the grid\n",
        "            if action == \"LEFT\": ny = max(0, y - 1)              # Move left, staying inside the grid\n",
        "            if action == \"RIGHT\":ny = min(self.n - 1, y + 1)     # Move right, staying inside the grid\n",
        "            self.agent_pos = (nx, ny)           # Update the agent's position\n",
        "            self.score -= 1                     # Small movement penalty\n",
        "        else:\n",
        "            self.score -= 1                     # Penalize unknown actions to keep policy sensible\n",
        "        return self.perceive()                  # Return the new observation\n",
        "\n",
        "# --- Policies ---------------------------------------------------------------\n",
        "\n",
        "def random_agent(obs):\n",
        "    \"\"\"Return a random action, ignoring the observation (baseline).\"\"\"\n",
        "    return random.choice([\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"CLEAN\"])  # Uniform random choice\n",
        "\n",
        "def reflex_agent(obs):\n",
        "    \"\"\"Clean if dirty; otherwise move randomly (simple reflex).\"\"\"\n",
        "    if obs[\"dirty\"]:                      # If the sensor says current cell is dirty\n",
        "        return \"CLEAN\"                    # Then clean it\n",
        "    return random.choice([\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"])  # Else move randomly\n",
        "\n",
        "def run(agent_fn, steps=100, seed=0):\n",
        "    \"\"\"Simulate an agent for a fixed number of steps and return the final score.\"\"\"\n",
        "    env = GridWorld(seed=seed)            # Create a fresh environment per run\n",
        "    for _ in range(steps):                # Repeat for the given number of steps\n",
        "        obs = env.perceive()              # Read the current observation\n",
        "        action = agent_fn(obs)            # Choose an action using the policy\n",
        "        env.step(action)                  # Apply the action to the environment\n",
        "    return env.score                      # Return total score as performance measure\n",
        "\n",
        "# Quick experiment: average scores over 5 seeds for the two base policies\n",
        "for fn in [random_agent, reflex_agent]:             # Iterate over the two policy functions\n",
        "    scores = [run(fn, seed=s) for s in range(5)]    # Run each policy with seeds 0..4\n",
        "    print(fn.__name__, \"avg score:\", sum(scores)/len(scores))  # Print the average score\n",
        "def model_based_agent_factory():\n",
        "    \"\"\"\n",
        "    Factory that returns a stateful agent function.\n",
        "    The agent keeps an internal model (memory) of:\n",
        "      - Which cells it has already visited\n",
        "      - Whether those cells were dirty when visited\n",
        "      - Its current believed position (since the real env hides it)\n",
        "    It uses this model to avoid re-cleaning clean cells and to systematically\n",
        "    explore the grid instead of moving randomly.\n",
        "    \"\"\"\n",
        "    # ---------- Persistent state (lives between calls) ----------\n",
        "    visited = set()                    # Set of (x,y) positions the agent has been to\n",
        "    cleaned = set()                    # Set of (x,y) positions that were cleaned\n",
        "    believed_pos = [0, 0]              # Agent's internal belief of its own position\n",
        "    move_cycle = [\"RIGHT\", \"DOWN\", \"LEFT\", \"UP\"]   # Deterministic exploration order\n",
        "    cycle_idx = 0                      # Index into the move_cycle\n",
        "\n",
        "    def model_based_agent(obs):\n",
        "        nonlocal believed_pos, cycle_idx   # Allow modification of these variables\n",
        "\n",
        "        x, y = believed_pos                # Current believed coordinates\n",
        "\n",
        "        # Step 1: Always clean if the current square is dirty\n",
        "        if obs[\"dirty\"]:\n",
        "            cleaned.add((x, y))            # Remember we cleaned this cell\n",
        "            visited.add((x, y))\n",
        "            return \"CLEAN\"\n",
        "\n",
        "        # Step 2: We are on a clean cell ‚Üí mark as visited (if not already)\n",
        "        visited.add((x, y))\n",
        "\n",
        "        # Step 3: Choose next move using deterministic cycle (systematic sweep)\n",
        "        # This gives much better coverage than pure random moves\n",
        "        for _ in range(4):                             # Try up to 4 directions\n",
        "            move = move_cycle[cycle_idx]\n",
        "            cycle_idx = (cycle_idx + 1) % 4            # Advance cycle\n",
        "\n",
        "            # Compute candidate new position\n",
        "            nx, ny = x, y\n",
        "            if move == \"RIGHT\":  ny += 1\n",
        "            if move == \"LEFT\":   ny -= 1\n",
        "            if move == \"DOWN\":   nx += 1\n",
        "            if move == \"UP\":     nx -= 1\n",
        "\n",
        "            # Do not move into cells we *know* are already clean (avoid useless moves)\n",
        "            if (nx, ny) in cleaned and (nx, ny) in visited:\n",
        "                continue                               # Skip this direction\n",
        "\n",
        "            # Otherwise this move looks promising ‚Üí take it\n",
        "            believed_pos = [nx, ny]                    # Update believed position\n",
        "            return move\n",
        "\n",
        "        # Fallback: if all 4 directions are known clean, just pick any valid move\n",
        "        # (this can still happen near the end when most of the grid is clean)\n",
        "        valid_moves = []\n",
        "        if y < 4: valid_moves.append(\"RIGHT\")\n",
        "        if y > 0: valid_moves.append(\"LEFT\")\n",
        "        if x < 4: valid_moves.append(\"DOWN\")\n",
        "        if x > 0: valid_moves.append(\"UP\")\n",
        "        return random.choice(valid_moves or [\"RIGHT\"])\n",
        "\n",
        "    return model_based_agent       # Return the stateful function\n",
        "\n",
        "\n",
        "# --- Evaluation helper (already provided, kept for completeness) ---\n",
        "def evaluate(agent_fn, trials=30):\n",
        "    \"\"\"Run the given agent across many random seeds and collect scores.\"\"\"\n",
        "    results = []\n",
        "    for i in range(trials):\n",
        "        score = run(agent_fn, seed=i, steps=100)   # 100 steps as in original lab\n",
        "        results.append(score)\n",
        "    return results\n",
        "\n",
        "\n",
        "# === Run comparison ===\n",
        "random_scores = evaluate(random_agent)\n",
        "reflex_scores = evaluate(reflex_agent)\n",
        "model_scores  = evaluate(model_based_agent_factory())   # note: factory returns new agent each eval\n",
        "\n",
        "# === Plot results ===\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(random_scores,  alpha=0.6, bins=15, label=f\"Random (Œº={np.mean(random_scores):.1f})\")\n",
        "plt.hist(reflex_scores,  alpha=0.6, bins=15, label=f\"Reflex (Œº={np.mean(reflex_scores):.1f})\")\n",
        "plt.hist(model_scores,   alpha=0.6, bins=15, label=f\"Model-based (Œº={np.mean(model_scores):.1f})\")\n",
        "\n",
        "plt.xlabel(\"Total Score after 100 steps\")\n",
        "plt.ylabel(\"Frequency (out of 30 trials)\")\n",
        "plt.title(\"Performance Comparison of Three Agents\\n(higher score = better cleaning efficiency)\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Print numeric summary\n",
        "print(\"=== Average scores over 30 trials ===\")\n",
        "print(f\"Random agent     : {np.mean(random_scores):.2f} ¬± {np.std(random_scores):.2f}\")\n",
        "print(f\"Reflex agent     : {np.mean(reflex_scores):.2f} ¬± {np.std(reflex_scores):.2f}\")\n",
        "print(f\"Model-based agent: {np.mean(model_scores):.2f} ¬± {np.std(model_scores):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c0bb22d",
      "metadata": {
        "id": "2c0bb22d"
      },
      "outputs": [],
      "source": [
        "# TODO(Student): Model‚Äëbased agent + comparison plot (with line‚Äëby‚Äëline comments)\n",
        "\n",
        "def model_based_agent_factory():\n",
        "    \"\"\"Return an agent function that remembers visited cells (very simple model).\"\"\"\n",
        "    visited = set()                                   # 'visited' will store coordinates seen before\n",
        "    last_move = [\"RIGHT\", \"DOWN\", \"LEFT\", \"UP\"]       # A simple move preference order\n",
        "\n",
        "    def agent(obs, _cache={\"pos\": (0, 0)}):\n",
        "        # The '_cache' dict stores the last known position; this keeps state between calls.\n",
        "        # In this simple demo we will not compute the position here (GridWorld hides it),\n",
        "        # but we can still use 'visited' as a proxy memory if we had access to coordinates.\n",
        "        if obs.get(\"dirty\"):                          # If current cell is dirty\n",
        "            return \"CLEAN\"                            # Always clean first\n",
        "        # Otherwise, just cycle through moves to explore the grid deterministically.\n",
        "        move = last_move[0]                           # Take the first preferred move\n",
        "        last_move.append(last_move.pop(0))            # Rotate the preference list\n",
        "        return move                                   # Return the chosen move\n",
        "    return agent                                      # Return the stateful agent function\n",
        "\n",
        "# --- Evaluation helper (fully commented) ------------------------------------\n",
        "def evaluate(agent_fn, trials=30):\n",
        "    \"\"\"Run the given agent across many random seeds and collect scores.\"\"\"\n",
        "    results = []                                      # Empty list to hold the scores\n",
        "    for i in range(trials):                           # Loop over 'trials' seeds\n",
        "        score = run(agent_fn, seed=i)                 # Run one simulation with seed 'i'\n",
        "        results.append(score)                         # Append the score to the results list\n",
        "    return results                                    # Return the list of scores\n",
        "\n",
        "# Compute scores for all three policies\n",
        "random_scores = evaluate(random_agent)                # List of 30 scores for the random policy\n",
        "reflex_scores = evaluate(reflex_agent)                # List of 30 scores for the reflex policy\n",
        "model_scores = evaluate(model_based_agent_factory())  # List of 30 scores for the model-based policy\n",
        "\n",
        "# Plot histograms to compare score distributions\n",
        "plt.figure()                                         # Create a new figure\n",
        "plt.hist(random_scores, alpha=0.5, label=\"random\")   # Histogram for random agent\n",
        "plt.hist(reflex_scores, alpha=0.5, label=\"reflex\")   # Histogram for reflex agent\n",
        "plt.hist(model_scores, alpha=0.5, label=\"model-based\")# Histogram for model-based agent\n",
        "plt.legend()                                         # Show legend with labels\n",
        "plt.xlabel(\"score\")                                  # X-axis label\n",
        "plt.ylabel(\"frequency\")                              # Y-axis label\n",
        "plt.title(\"Agent score distribution (higher is better)\")  # Plot title\n",
        "plt.show()                                           # Display the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e665821",
      "metadata": {
        "id": "7e665821"
      },
      "source": [
        "### Reflection\n",
        "- What **performance measure** did we implicitly design with our scoring?  \n",
        "- Which policy is most **rational** under this measure? Why?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
